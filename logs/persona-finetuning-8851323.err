`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:00<00:00, 25.80it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 37.08it/s]
Map:   0%|          | 0/366 [00:00<?, ? examples/s]Map:   1%|          | 3/366 [00:00<00:15, 24.09 examples/s]Map:   2%|▏         | 6/366 [00:00<00:13, 25.97 examples/s]Map:   3%|▎         | 11/366 [00:00<00:10, 33.88 examples/s]Map:   5%|▍         | 17/366 [00:00<00:09, 35.19 examples/s]Map:   6%|▌         | 21/366 [00:00<00:09, 35.12 examples/s]Map:   7%|▋         | 27/366 [00:00<00:08, 38.70 examples/s]Map:   8%|▊         | 31/366 [00:01<00:14, 23.53 examples/s]Map:   9%|▉         | 34/366 [00:01<00:14, 23.47 examples/s]Map:  11%|█         | 40/366 [00:01<00:11, 28.41 examples/s]Map:  13%|█▎        | 46/366 [00:01<00:09, 32.06 examples/s]Map:  15%|█▍        | 54/366 [00:01<00:08, 38.76 examples/s]Map:  17%|█▋        | 61/366 [00:01<00:07, 43.39 examples/s]Map:  18%|█▊        | 67/366 [00:01<00:07, 41.61 examples/s]Map:  20%|█▉        | 73/366 [00:02<00:07, 37.28 examples/s]Map:  21%|██        | 77/366 [00:02<00:08, 34.65 examples/s]Map:  22%|██▏       | 82/366 [00:02<00:08, 35.29 examples/s]Map:  23%|██▎       | 86/366 [00:02<00:09, 28.73 examples/s]Map:  25%|██▍       | 91/366 [00:02<00:09, 30.54 examples/s]Map:  26%|██▌       | 96/366 [00:02<00:09, 28.40 examples/s]Map:  28%|██▊       | 102/366 [00:03<00:08, 32.35 examples/s]Map:  29%|██▉       | 107/366 [00:03<00:08, 31.90 examples/s]Map:  31%|███       | 113/366 [00:03<00:07, 34.93 examples/s]Map:  33%|███▎      | 119/366 [00:03<00:06, 38.20 examples/s]Map:  34%|███▎      | 123/366 [00:03<00:06, 38.24 examples/s]Map:  35%|███▌      | 129/366 [00:03<00:05, 42.22 examples/s]Map:  37%|███▋      | 135/366 [00:03<00:06, 35.13 examples/s]Map:  38%|███▊      | 139/366 [00:04<00:08, 27.60 examples/s]Map:  39%|███▉      | 143/366 [00:04<00:08, 27.32 examples/s]Map:  41%|████      | 149/366 [00:04<00:06, 31.57 examples/s]Map:  42%|████▏     | 153/366 [00:04<00:06, 31.62 examples/s]Map:  44%|████▍     | 161/366 [00:04<00:04, 41.83 examples/s]Map:  45%|████▌     | 166/366 [00:04<00:04, 41.30 examples/s]Map:  48%|████▊     | 175/366 [00:05<00:03, 49.18 examples/s]Map:  49%|████▉     | 181/366 [00:05<00:03, 48.69 examples/s]Map:  52%|█████▏    | 190/366 [00:05<00:04, 42.56 examples/s]Map:  54%|█████▎    | 196/366 [00:05<00:03, 42.82 examples/s]Map:  55%|█████▍    | 201/366 [00:05<00:05, 27.66 examples/s]Map:  57%|█████▋    | 208/366 [00:06<00:04, 31.60 examples/s]Map:  58%|█████▊    | 214/366 [00:06<00:04, 35.41 examples/s]Map:  60%|██████    | 221/366 [00:06<00:03, 41.05 examples/s]Map:  62%|██████▏   | 227/366 [00:06<00:03, 41.17 examples/s]Map:  64%|██████▎   | 233/366 [00:06<00:03, 44.11 examples/s]Map:  65%|██████▌   | 238/366 [00:06<00:03, 39.83 examples/s]Map:  66%|██████▋   | 243/366 [00:06<00:03, 37.41 examples/s]Map:  67%|██████▋   | 247/366 [00:06<00:03, 37.24 examples/s]Map:  69%|██████▊   | 251/366 [00:07<00:03, 34.18 examples/s]Map:  70%|██████▉   | 256/366 [00:07<00:03, 30.61 examples/s]Map:  72%|███████▏  | 262/366 [00:07<00:03, 32.39 examples/s]Map:  73%|███████▎  | 268/366 [00:07<00:02, 35.66 examples/s]Map:  75%|███████▍  | 273/366 [00:07<00:02, 36.76 examples/s]Map:  76%|███████▌  | 278/366 [00:07<00:02, 37.62 examples/s]Map:  78%|███████▊  | 284/366 [00:08<00:02, 39.53 examples/s]Map:  79%|███████▉  | 289/366 [00:08<00:01, 40.96 examples/s]Map:  81%|████████  | 297/366 [00:08<00:01, 44.24 examples/s]Map:  83%|████████▎ | 303/366 [00:08<00:01, 36.25 examples/s]Map:  84%|████████▍ | 308/366 [00:08<00:01, 36.69 examples/s]Map:  85%|████████▌ | 312/366 [00:08<00:01, 29.20 examples/s]Map:  87%|████████▋ | 318/366 [00:09<00:01, 28.14 examples/s]Map:  89%|████████▉ | 326/366 [00:09<00:01, 34.04 examples/s]Map:  90%|█████████ | 331/366 [00:09<00:01, 34.36 examples/s]Map:  92%|█████████▏| 336/366 [00:09<00:00, 36.02 examples/s]Map:  93%|█████████▎| 340/366 [00:09<00:00, 31.89 examples/s]Map:  94%|█████████▍| 344/366 [00:09<00:00, 32.42 examples/s]Map:  96%|█████████▌| 351/366 [00:09<00:00, 37.89 examples/s]Map:  97%|█████████▋| 355/366 [00:10<00:00, 34.74 examples/s]Map:  98%|█████████▊| 360/366 [00:10<00:00, 37.39 examples/s]Map: 100%|█████████▉| 365/366 [00:10<00:00, 33.87 examples/s]Map: 100%|██████████| 366/366 [00:11<00:00, 31.72 examples/s]
Filter:   0%|          | 0/366 [00:00<?, ? examples/s]Filter: 100%|██████████| 366/366 [00:02<00:00, 138.10 examples/s]Filter: 100%|██████████| 366/366 [00:02<00:00, 137.95 examples/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/138 [00:00<?, ?it/s]/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/orcd/home/002/robinna/PGG-finetuning/Finetuning/train_persona_type_sft.py", line 252, in <module>
    main()
  File "/orcd/home/002/robinna/PGG-finetuning/Finetuning/train_persona_type_sft.py", line 247, in main
    trainer.train()
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/transformers/trainer.py", line 2328, in train
    return inner_training_loop(
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/transformers/trainer.py", line 2672, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/transformers/trainer.py", line 4060, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/accelerate/accelerator.py", line 2734, in backward
    loss.backward(**kwargs)
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/robinna/.conda/envs/llm_conda/lib/python3.9/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
  0%|          | 0/138 [00:01<?, ?it/s]
